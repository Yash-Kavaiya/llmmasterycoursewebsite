# 30-Days-LLM-Mastery-Course üöÄ

Dive into the world of Large Language Models (LLMs) with our comprehensive course! Learn about transformers, tokenizers, and cutting-edge NLP techniques. 

## Course Overview

Embark on an exciting journey into the realm of Large Language Models with Neural Hacks with Vasanth! This course covers everything from beginner concepts to advanced state-of-the-art techniques.

### üìö Course Highlights:

- Comprehensive coverage of LLM concepts
- In-depth exploration of transformers and their history
- Practical insights into tokenizers and their applications
- Hands-on coding sessions for building and fine-tuning models
- Advanced topics like preference alignment, quantization, and deployment

### üóìÔ∏è Course Structure:

- 30-day intensive program
- Daily video uploads
- Mix of theory, coding, and practical applications

### üîë Key Topics Covered:

- Transformer architectures and their evolution
- GPT family and decoder models
- Pre-training and fine-tuning pipelines
- Advanced techniques: LORA, DPO, quantization, and distillation
- Real-world applications and deployment strategies

### üë®‚Äçüè´ Who Should Attend:

This course is perfect for ML enthusiasts, NLP practitioners, and anyone looking to dive deep into the world of LLMs. Whether you're a beginner or an experienced professional, there's something for everyone!

## Daily Course Content

| Day | Content |
|-----|---------|
| 1   | Course Introduction, Transformers History, Transformer Types, "Attention Is All You Need" Paper Introduction |
| 2   | Transformer Explanation (continued) |
| 3   | Tokenizer Section, GPT Family Architecture and Overall Working |
| 4   | (Content not specified) |
| 5   | Positional Encoding, Types and Code |
| 6   | Attention, Types and Code |
| 7   | Other Components, Types and Code |
| 8   | Pretraining Pipeline Explanation, Data Collection |
| 9   | Data Processing, Creating PyTorch Dataset and DataLoader |
| 10  | Understanding LLAMA3 Architecture along with GPT Paper |
| 11  | Making a Miniature Version of the LLM - Part 1 |
| 12  | Making a Miniature Version of the LLM - Part 2 |
| 13  | Pretraining Mini-LLAMA, Training Pipeline Coding, Introduction to Strategies |
| 14  | Generation Hyperparameters Explanation |
| 15  | Decoding Strategies Explanation and Coding, Evaluation of Mini-LLAMA - Perplexity |
| 16  | Finetuning Mini-LLAMA for Text Classification |
| 17  | Post-Pretraining Pipeline Explanation, Detailed Introduction to SFT, Data Format |
| 18  | Instruction Tuning (SFT) - Data Collection Processing, Training |
| 19  | Finetuning with LORA Explanation |
| 20  | SFT Finetuning with Hugging Face Libraries |
| 21  | LLM Evaluation Theory and Code |
| 22  | DPO from Scratch & with Hugging Face with SFT |
| 23  | HF Model Quantization Complete Section along with KU Caching |
| 24  | Distillation & Pruning |
| 25  | LLM Deployment and Serving - Ollama, LitServe - How to Use, Live Demo |
| 26  | MOE Theory and Code |
| 27  | Model Merging Theory and Code |
| 28  | RAG Theory, Code, Real-time App with Deployment |
| 29  | Agents, Frameworks, Simple Agent till Deployment |
| 30  | Next Steps |


## Subscribe for Updates

Stay tuned for daily updates! Follow us on social media and use the hashtags #LLMCourse #NLP #MachineLearning to join the conversation.

## Contact and Resources

- üì∫ YouTube: [Neural Hacks with Vasanth](https://www.youtube.com/@NeuralHackswithVasanth)
- üíº LinkedIn: [Vasanth P](https://www.linkedin.com/in/vasanth-p-90826218b/)
- üíª GitHub: [Vasanthengineer4949](https://github.com/Vasanthengineer4949)

For more information, course materials, and updates, please visit our YouTube channel and follow the instructor on LinkedIn and GitHub.
